{"version":3,"sources":["utils/semantic.ts"],"names":["STOP_WORDS","Set","tokenize","text","toLowerCase","replace","split","map","t","trim","filter","has","termFreq","tokens","tf","forEach","norm","v","s","Object","keys","k","Math","sqrt","computeSimilarityScores","query","docs","getter","length","qTokens","docsTokens","d","JSON","stringify","idf","df","N","log","computeIdf","concat","qTf","qVec","docVecs","i","qNorm","scores","a","b","dot","item","score","Number","isFinite","sort"],"mappings":"4HAAA,+DAIA,IAAMA,EAAa,IAAIC,IAAI,CACzB,MAAM,KAAK,KAAK,QAAQ,KAAK,MAAM,IAAI,KAAK,OAAO,MAAM,KAAK,KAAK,KAAK,OAAO,OAAO,KAAK,KAAK,OAAO,KAAK,MAAM,KAAK,KAAK,MAAM,OAAO,MAAM,MAAM,OAAO,MAAM,MAAM,MAAM,SAG1KC,EAAW,SAACC,GAChB,OAAKA,EACEA,EACJC,cACAC,QAAQ,g5DAAsB,KAC9BC,MAAM,OACNC,KAAI,SAACC,GAAD,OAAOA,EAAEC,UACbC,QAAO,SAACF,GAAD,OAAOA,IAAMR,EAAWW,IAAIH,MANpB,IASdI,EAAW,SAACC,GAChB,IAAMC,EAA6B,GAEnC,OADAD,EAAOE,SAAQ,SAACP,GAAQM,EAAGN,IAAMM,EAAGN,IAAM,GAAK,KACxCM,GAqBHE,EAAO,SAACC,GACZ,IAAIC,EAAI,EAER,OADAC,OAAOC,KAAKH,GAAGF,SAAQ,SAACM,GAAQH,GAAKD,EAAEI,GAAKJ,EAAEI,MACvCC,KAAKC,KAAKL,IAGNM,EAA0B,SAAUC,EAAeC,EAAWC,GACzE,IAAKF,IAAUC,GAAwB,IAAhBA,EAAKE,OAAc,OAAOF,EAEjD,IAAMG,EAAU3B,EAASuB,GACzB,GAAuB,IAAnBI,EAAQD,OAAc,OAAOF,EAGjC,IAAMI,EAAaJ,EAAKnB,KAAI,SAACwB,GAAD,OAAO7B,EAASyB,EAASA,EAAOI,GAAKC,KAAKC,UAAUF,OAE1EG,EAjCW,SAACJ,GAClB,IAAMK,EAA6B,GACnCL,EAAWf,SAAQ,SAACF,GACL,IAAIZ,IAAIY,GAChBE,SAAQ,SAACP,GAAQ2B,EAAG3B,IAAM2B,EAAG3B,IAAM,GAAK,QAE/C,IAAM4B,EAAIN,EAAWF,QAAU,EACzBM,EAA8B,GAEpC,OADAf,OAAOC,KAAKe,GAAIpB,SAAQ,SAACP,GAAQ0B,EAAI1B,GAAKc,KAAKe,IAAKD,GAAM,EAAID,EAAG3B,QAC1D0B,EAwBKI,CAAWR,EAAWS,OAAO,CAACV,KAEpCW,EAAM5B,EAASiB,GACfY,EAA+B,GACrCtB,OAAOC,KAAKoB,GAAKzB,SAAQ,SAACP,GAAQiC,EAAKjC,GAAKgC,EAAIhC,IAAM0B,EAAI1B,IAAM,MAEhE,IAAMkC,EAAkD,GACxDZ,EAAWf,SAAQ,SAACF,EAAQ8B,GAC1B,IAAM7B,EAAKF,EAASC,GACdI,EAA4B,GAClCE,OAAOC,KAAKN,GAAIC,SAAQ,SAACP,GAAQS,EAAET,GAAKM,EAAGN,IAAM0B,EAAI1B,IAAM,MAC3DkC,EAAQC,GAAK1B,KAGf,IAAM2B,EAAQ5B,EAAKyB,IAAS,EAEtBI,EAASnB,EAAKnB,KAAI,SAACwB,EAAGY,GAC1B,IAAM1B,EAAIyB,EAAQC,GACZzB,EAvCE,SAAC4B,EAA2BC,GACtC,IAAI7B,EAAI,EAER,OADAC,OAAOC,KAAK0B,GAAG/B,SAAQ,SAACM,GAAY0B,EAAE1B,KAAIH,GAAK4B,EAAEzB,GAAK0B,EAAE1B,OACjDH,EAoCK8B,CAAIP,EAAMxB,IAAM2B,GAAS5B,EAAKC,IAAM,IAC9C,MAAO,CAAEgC,KAAMlB,EAAGmB,MAAOC,OAAOC,SAASlC,GAAKA,EAAI,MAMpD,OAFA2B,EAAOQ,MAAK,SAACP,EAAGC,GAAJ,OAAUA,EAAEG,MAAQJ,EAAEI,SAE3BL,EAAOtC,KAAI,SAACW,GAAD,OAAOA,EAAE+B","file":"static/js/6.4d54a6d5.chunk.js","sourcesContent":["// Simple client-side semantic ranking utility using TF-IDF and cosine similarity.\n// This is a lightweight AI-like implementation that reorders fetched GitHub\n// repositories by relevance to a descriptive query.\n\nconst STOP_WORDS = new Set([\n  'the','is','at','which','on','and','a','an','with','for','to','of','in','that','this','it','by','from','as','are','be','or','was','were','but','has','have','had','not','you','your'\n]);\n\nconst tokenize = (text: string): string[] => {\n  if (!text) return [];\n  return text\n    .toLowerCase()\n    .replace(/[\\p{P}$+<=>^`|~]/gu, ' ')\n    .split(/\\s+/)\n    .map((t) => t.trim())\n    .filter((t) => t && !STOP_WORDS.has(t));\n};\n\nconst termFreq = (tokens: string[]) => {\n  const tf: Record<string, number> = {};\n  tokens.forEach((t) => { tf[t] = (tf[t] || 0) + 1; });\n  return tf;\n};\n\nconst computeIdf = (docsTokens: string[][]) => {\n  const df: Record<string, number> = {};\n  docsTokens.forEach((tokens) => {\n    const seen = new Set(tokens);\n    seen.forEach((t) => { df[t] = (df[t] || 0) + 1; });\n  });\n  const N = docsTokens.length || 1;\n  const idf: Record<string, number> = {};\n  Object.keys(df).forEach((t) => { idf[t] = Math.log((N) / (1 + df[t])); });\n  return idf;\n};\n\nconst dot = (a: Record<string, number>, b: Record<string, number>) => {\n  let s = 0;\n  Object.keys(a).forEach((k) => { if (b[k]) s += a[k] * b[k]; });\n  return s;\n};\n\nconst norm = (v: Record<string, number>) => {\n  let s = 0;\n  Object.keys(v).forEach((k) => { s += v[k] * v[k]; });\n  return Math.sqrt(s);\n};\n\nexport const computeSimilarityScores = <T = any>(query: string, docs: T[], getter?: (d: T) => string) => {\n  if (!query || !docs || docs.length === 0) return docs;\n\n  const qTokens = tokenize(query);\n  if (qTokens.length === 0) return docs;\n\n  // Build tokens for each doc\n  const docsTokens = docs.map((d) => tokenize(getter ? getter(d) : JSON.stringify(d)));\n\n  const idf = computeIdf(docsTokens.concat([qTokens]));\n\n  const qTf = termFreq(qTokens);\n  const qVec: Record<string, number> = {};\n  Object.keys(qTf).forEach((t) => { qVec[t] = qTf[t] * (idf[t] || 0); });\n\n  const docVecs: Record<number, Record<string, number>> = {};\n  docsTokens.forEach((tokens, i) => {\n    const tf = termFreq(tokens);\n    const v: Record<string, number> = {};\n    Object.keys(tf).forEach((t) => { v[t] = tf[t] * (idf[t] || 0); });\n    docVecs[i] = v;\n  });\n\n  const qNorm = norm(qVec) || 1;\n\n  const scores = docs.map((d, i) => {\n    const v = docVecs[i];\n    const s = dot(qVec, v) / (qNorm * (norm(v) || 1));\n    return { item: d, score: Number.isFinite(s) ? s : 0 };\n  });\n\n  // Sort by score desc, keep stable order for equal scores\n  scores.sort((a, b) => b.score - a.score);\n\n  return scores.map((s) => s.item);\n};\n"],"sourceRoot":""}